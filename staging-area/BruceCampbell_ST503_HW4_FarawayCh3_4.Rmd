---
title: "NCSU ST 503 HW 4"
subtitle: "Probems 3.2, 3.4, 3.5, 3.6, 4.2 Faraway, Julian J. Linear Models with R, Second Edition Chapman & Hall / CRC Press."
author: "Bruce Campbell"
date: "`r format(Sys.time(), '%d %B, %Y')`"
fontsize: 12pt
header-includes:
   - \usepackage{bbm}
output: pdf_document
---

---
```{r setup, include=FALSE,echo=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dev = 'pdf')
knitr::opts_chunk$set(cache=TRUE)
knitr::opts_chunk$set(tidy=TRUE)
knitr::opts_chunk$set(prompt=FALSE)
knitr::opts_chunk$set(fig.height=5)
knitr::opts_chunk$set(fig.width=7)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_knit$set(root.dir = ".")
library(latex2exp)   
library(pander)
library(ggplot2)
library(ggplot2)
library(GGally)
library(broom)
library(printr)
```

## Problem 3.2

_Thirty samples of cheddar cheese were analyzed for their content of acetic acid, hydrogen sulfide and lactic acid. Each sample was tasted and scored by a panel of judges and the average taste score produced. Use the cheddar dataset_

### (a) Fit a regression model with taste as the response and the three chemical contents as predictors. Identify the predictors that are statistically significant at the 5% level.

```{r, echo = FALSE}
data(cheddar, package="faraway")
```

```{r, echo=TRUE}
lm.fit <- lm(taste ~ Acetic + H2S + Lactic, data=cheddar)
```

```{r, echo=FALSE}
tidy(lm.fit)
kable(data.frame(r.squared = summary(lm.fit)$r.squared))
```

We see that $H2S$ and $Lactic$ are significant to the $5\%$ level. 

### (b) Acetic and H2S are measured on a log scale. Fit a linear model where all three predictors are measured on their original scale. Identify the predictors that are statistically significant at the 5% level for this model.

To undo the log transform we need the base - this is not specified in the help section for the data set. Since we're dealing with chemical concentration data, and based on part e) we will assume that $Acetic$ and $H2S$ are measured on a $Log_{e}$ scale.  

```{r , echo=TRUE}
lm.fit.exp <- lm(taste ~ I(exp(1)^Acetic) + I(exp(1)^H2S) + Lactic, data=cheddar)
```

```{r echo=FALSE}
tidy(lm.fit.exp)
library(knitr)
kable( data.frame(rsquared = summary(lm.fit.exp)$r.squared) , caption = "taste ~ I(exp(1)^Acetic) + I(exp(1)^H2S) + Lactic")

```

We see that now only $Lactic$ is significant at the $5\%$ level.  $H2S$ is significant at $10\%$.   We thought this could be due to numerical issues in the QR - to test that out we took the transformed data set, standardize it and fit that. 

For comparison on the effect of scaling we also fit the scaled model without the inverse log transform. The scaled inverse log transformed model had  $H2S$ and $Lactic$ significant to the $5\%$ level.

```{r, echo=FALSE}
# Test code to see the effects of scaling each model 
#df <- cheddar
#df <-data.frame(scale(df))
#lm.fit.scaled <- lm(taste ~ Acetic + H2S + Lactic, data=df)
#summary(lm.fit.scaled)

#df$Acetic <- exp(1)^df$Acetic
#df$H2S <- exp(1)^df$H2S
#df <-data.frame(scale(df))
#lm.fit.transform.scaled <- lm(taste ~ Acetic + H2S + Lactic, data=df)
#summary(lm.fit.transform.scaled)
```


### (c) Can we use an F-test to compare these two models? Explain. Which model provides a better fit to the data? Explain your reasoning. 

We can not use an F-test to compare these models since they are not nested. The model fit in $ln$ scale is a better fit to the data based on the $R^2$ criteria. 

### (d) If H2S is increased 0.01 for the model used in (a), what change in the taste would be expected? 

For the model fit in part a) we saw that $\beta_{H2S} = 3.9118$ this means that keeping all other variables constant and increasing $H2S$ by $0.01$ increases taste by $0.039118$.  We can verify this is the case numerically on an example data element from the training set. 

```{r}
data.sample<- sample(nrow(cheddar),1)
data.element <- cheddar[data.sample,]
data.element$taste <-NULL
data.element <- as.matrix(cbind(intercept=1,data.element))
beta.hat <- as.matrix( lm.fit$coefficients)
pander(data.frame(data.element), caption ="Data sample")
response.orig <- (data.element) %*% beta.hat    
#change the of our data element H2S by +0.01  
data.element[1,3] <- data.element[1,3] + 0.01
pander(data.frame(data.element), caption ="Data sample data element H2S increased by +0.01")
response.mod <- (data.element) %*% beta.hat
pander(data.frame(response.difference = (response.mod - response.orig)))
```

### (e) What is the percentage change in H2S on the original scale corresponding to an additive increase of 0.01 on the (natural) log scale?

Let our log concentration be $\alpha$ then $e^\alpha$ is our concentration in the original scale. A $\delta$ change in the log scale H2S results in a concentration of $e^{\alpha + \delta}$

The percent change is 
$$( \frac{e^{\alpha + \delta}- e^\alpha}{e^{\alpha}} ) * 100 \% =  ( e^{ \delta}-1 ) * 100 \%$$
In our case $\delta=0.01$ and the percent change is ```r exp(1)^0.01 *100```

## Problem 3.3

_Using the teengamb data, fit a model with gamble as the response and the other variables as predictors._

### (a) Which variables are statistically significant at the 5% level?

```{r,echo=FALSE}
rm(list = ls())
data(teengamb, package="faraway")
```

```{r}
lm.fit <- lm(gamble ~ sex+status+income+verbal, data=teengamb)
```

```{r, echo=FALSE}
tidy(lm.fit)
```

We see that $gender$ and $income$ are both significant at the $5\%$ level.  

### (b) What interpretation should be given to the coefficient for sex?

The variable $sex$ is encoded $0=male, 1=female$ and the coefficient for it $\beta_{sex} = -22.118$.  
This means that when all the other variables are held constant and the gender changes from male to female that
there will be a $-22.118$ change in $gamble$.

### (c) Fit a model with just income as a predictor and use an F-test to compare it to the full model.

```{r}
lm.fit.income <- lm(gamble ~ income, data=teengamb)
```

The reduced model $gamble \sim income$
```{r,echo=FALSE}
tidy(lm.fit.income)
```

Results of the F-test

```{r,echo=FALSE}
tidy(anova(lm.fit.income ,lm.fit))
```


Based on the p-value of the F-statistic we do have enough evidence to reject the null hypothesis that the models are equivalent in the variance explained via the RSS statistic.  We claim that the full model is better based on the RSS criteria.

##Problem 3.4

We are using the sat data for this problem. 
```{r,echo=FALSE}
rm(list = ls())
data(sat, package = "faraway")
```
### (a) Fit a model with total sat score as the response and expend, ratio and salary as predictors. Test the hypothesis that $\beta_{salary} = 0$. Test the hypothesis that $\beta_{salary} = \beta_{ratio} = \beta_{expend} = 0$. Do any of these predictors have an effect on the response? 

```{r}
lm.fit <- lm(total ~ expend+ratio+ salary, data=sat)
tidy(lm.fit)
```

We see that salary is significant at the $\alpha= 10\%$ level.  

```{r}
lm.fit.reduced <- lm(total ~ expend+ratio, data=sat)
anova(lm.fit.reduced,lm.fit)
```
We see that the F-statistic has a p-value of $0.0667$ - this is the same as the p-value for the t-statistic given above for the coefficient $\beta_{salary}$ 

Test $H_0 \; : \; \beta_{salary} = \beta_{ratio} = \beta_{expend} = 0$

```{r}
lm.fit.null <- lm(total ~ 1, data=sat)

anova(lm.fit.null,lm.fit)
```

Based on the F-statistic we have enough evidence to reject the null hypothesis that all coefficients are zero.  We claim at least one predictor has an effect on the response. 

### (b) Now add takers to the model. Test the hypothesis that $\beta_{takers} = 0$. Compare this model to the previous one using an F-test. Demonstrate that the F-test and t-test here are equivalent.

Fit the model $total \sim expend+ratio+ salary + takers$
```{r}
lm.fit <- lm(total ~ expend+ratio+ salary + takers, data=sat)
tidy(lm.fit)
```

Fir the model $total \sim expend+ratio+ salary$ and perform the F-test.
```{r}
lm.fit.reduced <- lm(total ~ expend+ratio+ salary, data=sat)
anova(lm.fit.reduced,lm.fit)
```

Just as above we see that the F-statistic for the reduced model has a p-value that is the same as the p-value for the t-statistic given above for the coefficient $\beta_{takers}$ 

##Problem 3.5 $R^2$ and the F-test

###Find a formula relating R 2 and the F-test for the regression. 

Let $\Omega$ be the parameter space for a model in $p$ dimensions and $\omega$ be the parameter space for a model in $q$ dimensions. 

$$R^2_{\Omega} = 1 - \frac{RSS_{\Omega}}{TSS}$$
$$R^2_{\omega} = 1 - \frac{RSS_{\omega}}{TSS}$$
Solving for $TSS$ in the first case we have that $TSS=\frac{RSS_{\Omega}}{(1-R^2_{\Omega})}$ and putting this into the the expression for $R^2_{\omega}$ 

$$R^2_{\omega} = 1-\frac{RSS_\omega}{RSS_\Omega} (1-R^2_\Omega)  \implies $$

$$ \frac{RSS_\omega}{RSS_\Omega}R^2_\Omega - R^2_\omega = \frac{RSS_\omega-RSS_\Omega}{RSS_\Omega}  \implies$$

$$ (\frac{RSS_\omega}{RSS_\Omega}R^2_\Omega - R^2_\omega) \frac{dF_\Omega}{df_\Omega-df_\omega} = \frac{(RSS_\omega-RSS_\Omega) /(df_\Omega-df_\omega) }{RSS_\Omega / df_\Omega}  \sim F_{df_\Omega-df_\omega,n -df_\Omega}$$

Another way to think about $R^2$ is that it says something about a model in $\Omega$ versus the null model $y \sim \beta_0$.  $TSS=\sum (y_i -\bar{y})^2$ in this case will be $RSS_{\omega_0}$ - the sum of square residuals for the null model. In this case $df_\omega=1$ and we can manipulate the expression for $R^2 = 1-\frac{RSS_\Omega}{RSS_{\omega_0}}$ directly to get

$$ 1-R^2 =  \frac{RSS_\Omega}{RSS_{\omega_0}}  \implies 1-\frac{RSS_\Omega}{RSS_{\omega_0}} = \frac{R^2}{(1-R^2)}$$
and that _for the sace of comparing a full model against the null model_ we have


$$ \frac{R^2}{(1-R^2)} \frac{p}{p-1} =  \frac{(RSS_{\omega_0}-RSS_\Omega) /(p-1) }{RSS_\Omega / p}  \sim F_{p-1,n -p}$$


##Problem 3.6 MBA Students

_Thirty-nine MBA students were asked about happiness and how this related to their income and social life. The data are found in faraway::happy._

Note, pay attention to warnings in R! GGally has a happy data set as well.  

```{r,echo=FALSE}
rm(list = ls())
library(faraway)
data(happy, package = "faraway")
```

###Fit a regression model with happy as the response and the other four variables as predictors. 
```{r, echo=FALSE}
lm.fit <- lm(happy ~ money+sex+love+work, data = happy)
summary(lm.fit)
```
###(a) Which predictors were statistically significant at the 1% level? 

We see that money and love are significant at the $1\%$ level. 

###(b) Use the table function to produce a numerical summary of the response. What assumption used to perform the t-tests seems questionable in light of this summary? 

```{r}
table(happy$happy)
#hist(happy$happy)
#plot(lm.fit)
```

Wow, I have NO idea what this question is asking me. The assumptions we make are;
* Linear relationship
* Multivariate normality
* No or little multicollinearity
* No auto-correlation
* Homoscedasticity - the variance around the regression line is the same for all values of the predictors

All of these assumptions involve elements beyond the distribution of the measured responses $ \{ y_i \} $. 


###(c) Use the permutation procedure described in Section 3.3 to test the significance of the money predictor. 

```{r, echo=TRUE}
#summary(lm.fit)$coef[2,]# Est, sterr,t-stat,pval for the  
nreps <- 4000 
tstats <- numeric(nreps)
for(i in 1:nreps)
{ 
  lm.resample.money <- lm (happy ~ sample(money)+sex+love+work, data = happy) 
  tstats [i] <- summary (lm.resample.money )$coef[2,3] #Get the tstatistic for this resmapled model 
} 
simulated.pvalue <- mean(abs(tstats)  > abs(summary(lm.fit)$coef[2,])) #Calculate the proportion that exceed the original t-statistic

pander(data.frame(simulated.pvalue=simulated.pvalue))
```
WE see that the simulated pvalue based on resampling the $money$ predictor is very close the value we got from performing the t-test on $\beta_{money}$

```{r}
pvalue.money <-summary(lm.fit)$coef[2,4]
pander(data.frame(pvalue.money = pvalue.money))
```

### (d) AND (e) Plot a histgram of the permutation t-statistics. Overlay an appropriate t-density

```{r}
hist(tstats,30,freq = FALSE)
grid <- seq(-4, 4, length = 300)
n <- nrow(happy)
p <- 4+1 # Four predictors plus the intercept
df <- n-p
hist(tstats,30,freq = FALSE)
curve(dt(x, df=df), add=TRUE,col='red')
```
 
### (f) Use the bootstrap procedure from Section 3.6 to compute 90% and 95% confidence intervals for $money$. Does zero fall within these confidence intervals? Are these results consistent with previous tests?

```{r}
nb <- 4000
coefmat <- matrix(NA,nb,5)
resids <- residuals(lm.fit)
preds <- fitted(lm.fit)
for(i in 1:nb)
  {
    booty <- preds + sample(resids, rep=TRUE)
    bmod <- update(lm.fit, booty ~ .)
    coefmat[i,] <- coef(bmod)
}
colnames(coefmat) <- c("Intercept", "money","sex","love","work")
coefmat <- data.frame(coefmat)
apply(coefmat,2,function(x) quantile(x,c(0.05,0.95)))
```

We see that for a significance of $\alpha=10\%$ that we have enough evidence to reject the null hypothesis that the coefficient for $money$ is zero.  This is the same result for the permutation test and for the t-test that is performed as part of R's ```lm.summary``` routine.  Now we look at the $95$ confidence interval.

```{r}
apply(coefmat,2,function(x) quantile(x,c(0.025,0.975)))

```

Since $0$ is in the interval for $money$; at a significance of $\alpha=5\%$,  with the data at hand, we do _not_ have enough evidence to reject the null hypothesis that the coefficient for $money$ is zero in the linear model $happy \sim money+sex+love+work$ 

## Problem 4.2 - prediction with the teengamb data set.

Using the teengamb data, fit a model with gamble as the response and the other variables as predictors. 

```{r}
rm(list = ls())
data(teengamb, package = "faraway")
lm.fit <- lm(gamble ~ .,data=teengamb)
```

###(a) Predict the amount that a male with average (given these data) status, income and verbal score would gamble along with an appropriate 95% CI. 

```{r}
x <- model.matrix(lm.fit)
x0 <- apply(x,2,mean)
#The question asks for a male and here we set that value 
x0['sex'] <-0
#predict(lm.fit,new=data.frame(t(x0)))
pi<- predict(lm.fit,new=data.frame(t(x0)),interval="confidence",level = .95)
pi
pander(data.frame(pi.width=pi[3]-pi[2]), caption = "Confidence interval width")
```

###(b) Repeat the prediction for a male with maximal values (for this data) of status, income and verbal score. Which CI is wider and why is this result expected? 

```{r}
x <- model.matrix(lm.fit)
x0 <- apply(x,2,max)
#The question asks for a male and here we set that value 
x0['sex'] <-0
#predict(lm.fit,new=data.frame(t(x0)))
pi<- predict(lm.fit,new=data.frame(t(x0)),interval="confidence",level = .95)
pi
pander(data.frame(pi.width=pi[3]-pi[2]), caption = "Confidence interval width")
```


(c) Fit a model with sqrt(gamble) as the response but with the same predictors. Now predict the response and give a 95% prediction interval for the individual in (a). Take care to give your answer in the original units of the response. 

```{r}
lm.fit <- lm(sqrt(gamble) ~ .,data=teengamb)
x <- model.matrix(lm.fit)
x0 <- apply(x,2,mean)
x0['sex'] <-0
pi<- predict(lm.fit,new=data.frame(t(x0)),interval="confidence",level = .95)
#ORIG
pi.orig<-c(pi[1]^2,pi[2]^2,pi[3]^2)
pi.orig
pander(data.frame(pi.width=pi.orig[3]-pi.orig[2]), caption = "Confidence interval width")
```

The square root transform is known to stabilize variance and we see that in the smaller prediction interval.  

### (d) Repeat the prediction for the model in (c) for a female with status=20, income=1, verbal = 10. Comment on the credibility of the result.


```{r}
x0['sex'] <-1
x0['status'] <-20
x0['income'] <-1
x0['verbal'] <-10

pi<- predict(lm.fit,new=data.frame(t(x0)),interval="confidence",level = .95)
#ORIG
pi
pi.orig<-c(-pi[1]^2,pi[2]^2,pi[3]^2)
pi.orig
pander(data.frame(pi.width=pi.orig[3]-pi.orig[2]), caption = "Confidence interval width")

ggpairs(teengamb)

lm.fit <- lm(gamble ~ .,data=teengamb)
pi<- predict(lm.fit,new=data.frame(t(x0)),interval="confidence",level = .95)
pi

```