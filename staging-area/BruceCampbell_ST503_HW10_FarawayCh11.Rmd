---
title: "NCSU ST 503 HW 10"
subtitle: "Probems 11.1, 11.2, 11.3, and 11.4  Faraway, Julian J. Linear Models with R, Second Edition Chapman & Hall / CRC Press."
author: "Bruce Campbell"
date: "`r format(Sys.time(), '%d %B, %Y')`"
fontsize: 12pt
header-includes:
   - \usepackage{bbm}
output: pdf_document
---

---
```{r setup, include=FALSE,echo=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(dev = 'pdf')
knitr::opts_chunk$set(fig.height=5)
knitr::opts_chunk$set(fig.width=7)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
library(pander)
library(faraway)
library(ggplot2)
```

## 11.1 seatpos PCR analysis

Using the seatpos data, perform a PCR analysis with hipcenter as the response and HtShoes, Ht, Seated, Arm, Thigh and Leg as predictors. Select an appropriate number of components and give an interpretation to those you choose. Add Age and Weight as predictors and repeat the analysis. Use both models to predict the response for predictors taking these values: 

$$( HtShoes, Ht, Seated, Arm, Thigh, Leg,Age, Weight) =( 181.080, 178.560, 91.440, 35.640, 40.950, 38.790, 64.800, 263.700)$$



```{r}
rm(list = ls())
data(seatpos, package="faraway")
df <- seatpos
df.pca.inputs <-subset ( df,select = c("HtShoes", "Ht", "Seated", "Arm", "Thigh","Leg"))
mean.df <-apply(df.pca.inputs,2,mean  )
sd.df <-apply(df.pca.inputs,2,sd)

pca.seatpos <- prcomp(df.pca.inputs,scale. = TRUE)
summary(pca.seatpos)
```

We see that the first three PCA cpmonents account for 96.5% of the variance and the proportion of the variance in the third component is 0.8%.  We could choose to fit a regression model with the first two or three principal components. First we investigate the loadings on the first two principal components to see if we can discern any patterns that will allow for interpretation. Based on that we can decide how many components to put in the model.

```{r}
pander( data.frame(first.pc.loadings =round(pca.seatpos$rotation[,1], 3)), caption ="First Principal Component")
```

```{r}
pander( data.frame(first.pc.loadings =round(pca.seatpos$rotation[,2], 3)), caption ="Second Principal Component")
```

We see that the first component is an average size measure while the second is a contrast measure between $\{Arm,Thigh\}$ and $\{HtShoes,Ht,Seated,Leg\}$.

```{r}
pander( data.frame(first.pc.loadings =round(pca.seatpos$rotation[,3], 3)), caption ="Third Principal Component")
```

The third principal component is a contrast between $\{Arm,Leg\}$ and $\{HtShoes,Ht,Seated,Thigh\}$  We leave this out of the regression model. 

Here's a bubble plot of the first 2 components sized by the response. 

```{r}
scores <- data.frame(seatpos$hipcenter, pca.seatpos$x[,1:2])

qplot(x=PC1, y=PC2, data=scores )+ geom_point(aes(size = seatpos.hipcenter))
```

Now we perform the PCR on the first 2 components. 

```{r}
lm.pcr <- lm(seatpos$hipcenter ~ pca.seatpos$x[,1:2])
summary(lm.pcr)
```

### Full Model

```{r}
df.pca.inputs.full <-subset ( df,select = c("HtShoes", "Ht", "Seated", "Arm", "Thigh","Leg", "Age","Weight"))
mean.df.full <-apply(df.pca.inputs.full,2,mean  )
sd.df.full <-apply(df.pca.inputs.full,2,sd)
pca.seatpos.full <- prcomp(df.pca.inputs.full,scale. = TRUE)
summary(pca.seatpos.full)

pander( data.frame(first.pc.loadings =round(pca.seatpos.full$rotation[,1], 3)), caption ="First Principal Component")
pander( data.frame(first.pc.loadings =round(pca.seatpos.full$rotation[,2], 3)), caption ="Second Principal Component")
pander( data.frame(first.pc.loadings =round(pca.seatpos.full$rotation[,3], 3)), caption ="Third Principal Component")
scores.full <- data.frame(seatpos$hipcenter, pca.seatpos.full$x[,1:2])

qplot(x=PC1, y=PC2, data=scores.full )+ geom_point(aes(size = seatpos.hipcenter))

lm.pcr.full <- lm(seatpos$hipcenter ~ pca.seatpos.full$x[,1:2])
summary(lm.pcr.full)
```
We tried three PC's but did not achieve significant results for the this component's coefficient and we dropped that term from the model.  The First PC had the same interpretation while second PC added Age and Weight to the Arm ,Thigh part of the contrast $\{Arm,Thigh\}$ and $\{HtShoes,Ht,Seated,Leg\}$ from our first model.  Thus the second PC can be interpreted as a contrast between $\{Arm,Thigh, Age, Weight\}$ and $\{HtShoes,Ht,Seated,Leg\}$. 

To do the prediction we need to scale (we used scaling) and project the test point onto the the first two PCA.  We were also careful when creating the prediction data element to order the variables as they were in the rotation matrix.  We had some trouble with the predict function so we went ahead and calculated the predicted value manually.  First we scaled, then rotated, then took the first 2 components to calculate $\hat{\beta} \cdot \; x_0$ 

```{r, echo=TRUE}
DFTest <- data.frame( HtShoes=181.080, Ht=178.560, Seated=91.440, Arm=35.640, Thigh=40.950, Leg=38.7,Age=64.800, Weight=263.700)

x <- as.matrix(DFTest)

x <- (x-mean.df.full) / sd.df.full

R <- pca.seatpos.full$rotation 

x.r <- R %*% t(x)

pred.manual.comp  <- lm.pcr.full$coefficients["(Intercept)"] +  lm.pcr.full$coefficients["pca.seatpos.full$x[, 1:2]PC1"] * x.r[1,] +   lm.pcr.full$coefficients["pca.seatpos.full$x[, 1:2]PC2"] * x.r[2,]
names(pred.manual.comp) <- "predicted.hipcenter"

pander(data.frame(pred.manual.comp=pred.manual.comp), caption = "Predicted hipcenter for full data element")

```

Now we calculate the predicted hipcenter for the reduced data in a similar fashion- i.e. no $Age, Weight$. 

```{r}
DFTest <- data.frame( HtShoes=181.080, Ht=178.560, Seated=91.440, Arm=35.640, Thigh=40.950, Leg=38.7)

x <- as.matrix(DFTest)

x <- (x-mean.df) / sd.df

R <- pca.seatpos$rotation 

x.r <- R %*% t(x)

pred.manual.comp  <- lm.pcr$coefficients["(Intercept)"] +  lm.pcr$coefficients["pca.seatpos$x[, 1:2]PC1"] * x.r[1,] +   lm.pcr$coefficients["pca.seatpos$x[, 1:2]PC2"] * x.r[2,]

names(pred.manual.comp) <- "predicted.hipcenter"

pander(data.frame(pred.manual.comp=pred.manual.comp), caption = "Predicted hipcenter for model with no Age, Weight ")

```

We get a markedly different result in this case. 

## 11.2 PLS analysis with seatpos data 

Fit a PLS model to the seatpos data with hipcenter as the response and all other variables as predictors. Take care to select an appropriate number of components. Use the model to predict the response at the values of the predictors specified in the first question.

Based on our PCA modelling and some experimenting we choose to go with three components. 

```{r}
set.seed(123)
library(pls)
pls.mod <- plsr(hipcenter ~., data=seatpos, ncomp=3, validation ="CV") 
coefplot(pls.mod, ncomp=3, xlab="Frequency") 
plsCV <- RMSEP(pls.mod, estimate="CV") 
plot(plsCV,main="")
```

Now we predict the response for the test data.

```{r}
DFTest <- data.frame( HtShoes=181.080, Ht=178.560, Seated=91.440, Arm=35.640, Thigh=40.950, Leg=38.7,Age=64.800, Weight=263.700)

pred.manual.comp <-predict(pls.mod, DFTest, ncomp=3)


pander(data.frame(pred.manual.comp=pred.manual.comp), caption = "PLS predicted hipcenter")
```


## 11.3 Ridge regression with seatpos data 

Fit a ridge regression model to the seatpos data with hipcenter as the response and all other variables as predictors. Take care to select an appropriate amount of shrinkage. Use the model to predict the response at the values of the predictors specified in the first question.

First we make a few plots to see what the range of $\lambda$ should be.

```{r}
require(MASS) 
df <-seatpos
mean.df.full <-apply(df,2,mean  )
sd.df.full <-apply(df,2,sd)
df<-data.frame(scale(df, center = TRUE,scale = TRUE))
ridge.fit <- lm.ridge(hipcenter ~., df, lambda = seq(0, 5000, len=5000))
matplot(ridge.fit$lambda, coef(ridge.fit), type="l", xlab=expression(lambda) ,ylab=expression(hat(beta)),col=1)
```

Now we fit 500 models in the range $\lambda \in (0,100)$, find the minimum error model via cross validation, and plot the location of the $\lambda$ that minimizes the cross validation error on the coefficient plot. 

```{r}

ridge.fit <- lm.ridge(hipcenter ~., df, lambda = seq(0, 100, len=500))
lambda.min.loc <-which.min(ridge.fit$GCV)
lambda.min <- ridge.fit$GCV[lambda.min.loc]
matplot(ridge.fit$lambda, coef(ridge.fit), type="l", xlab=expression(lambda) ,ylab=expression(hat(beta)),col=1)
abline(v=22.24,col='red')
```

Here we predict the response for the ridge model with predictor values 

$$HtShoes=181.080\;, Ht=178.560\;, Seated=91.440\;, Arm=35.640\;Thigh=40.950;, Leg=38.7\;,Age=64.800\;, Weight=263.700$$

We scaled the data before fitting the ridge model.  We display the code below for applying the scaling to the predictors, predicting the fit from the optimal model determined by cross validation, and then undoing the scaling on the predicted response.  

```{r, echo=TRUE}
DFTest <- data.frame( HtShoes=181.080, Ht=178.560, Seated=91.440, Arm=35.640, Thigh=40.950, Leg=38.7,Age=64.800, Weight=263.700)

mean.pred <- c(mean.df.full["HtShoes"], mean.df.full["Ht"], mean.df.full["Seated"], mean.df.full["Arm"], mean.df.full["Thigh"],mean.df.full["Leg"], mean.df.full["Age"],mean.df.full["Weight"])

sd.pred <- c(sd.df.full["HtShoes"], sd.df.full["Ht"], sd.df.full["Seated"], sd.df.full["Arm"], sd.df.full["Thigh"],sd.df.full["Leg"], sd.df.full["Age"],sd.df.full["Weight"])

x <- as.matrix(DFTest)

x <- (x-mean.pred) / sd.pred

ypred <- cbind(1,as.matrix(x)) %*% coef(ridge.fit)[112,]

pred.manual.comp <- ypred*sd(seatpos$hipcenter) +mean(seatpos$hipcenter)

pander(data.frame(pred.manual.comp=pred.manual.comp), caption = "ridge Regression predicted hipcenter")

```


## 11.4 fat Analysis

Take the fat data, and use the percentage of body fat, siri, as the response and the other variables, except brozek and density as potential predictors. Remove every tenth observation from the data for use as a test sample. Use the remaining data as a training sample building the following models:
Use the models you find to predict the response in the test sample. Make a report on the performances of the models.


```{r}
rm(list = ls())
data(fat, package="faraway")
df<- fat
df <- df[ , -which(names(df) %in% c("brozek","density"))]
set.seed(123)
train <-  (1:nrow(fat) / 10) %%1==0

df.train <-df[train,]

df.test <-df[-train,]

```

### (a) Linear regression with all predictors 

```{r}
lm.fit <- lm(siri ~ .,data = df.train)
summary(lm.fit)

yhat.test <- predict(lm.fit,newdata = df.test, type="response")

mspe.lm <- mean((df.test$siri - yhat.test) ^ 2)

```

### (b) Linear regression with variables selected using AIC 

We plot the AIC for the models here and compare to the exhaustive search for reference. 

```{r}
library(leaps)
n <- nrow(df.train)
regsubsets.out <- regsubsets(siri ~ .,data=df.train,method = "exhaustive") 
rs <- summary(regsubsets.out) 
rs$which
AIC <- n*log(rs$rss/n) + (2:9)*2
plot(AIC ~ I(1:8), ylab="AIC", xlab="Number of Predictors")
```

We see that the model with the lowest AIC has 8 predictors.  The best 8 predictor model being $height + adipos + free + chest + abdom +  biceps + forearm + wrist$

Now we fit the model and calculate the MSPE 

```{r}
lm.fit <- lm(siri ~ height + adipos + free + chest + abdom +  biceps + forearm + wrist,data = df.train)
summary(lm.fit)

yhat.test <- predict(lm.fit,newdata = df.test, type="response")

mspe.lm.regsubsets <- mean((df.test$siri - yhat.test) ^ 2)

```

### (c) Principal component regression 

```{r}
df.pca.inputs <-df.train[ , -which(names(df.train) %in% c("siri"))]
mean.df <-apply(df.pca.inputs,2,mean  )
sd.df <-apply(df.pca.inputs,2,sd)

pca.results <- prcomp(df.pca.inputs,scale. = TRUE)
summary(pca.results)
s<- summary(pca.results)
plot(s$sdev,main = "Scree Plot for PCA")
```

Based on the scree plot we choose 5 principal components for our model. 

```{r}
lm.pcr <- lm(df.train$siri ~ pca.results$x[,1:15])
summary(lm.pcr)

df.test.predictors <- df.test[ , -which(names(df.test) %in% c("siri"))]
df.test.predictors <- scale(df.test.predictors,center = pca.results$center,scale = pca.results$scale)
df.test.predictors <- data.frame(df.test.predictors)
yhat.pca <-predict(lm.pcr,df.test.predictors)

mspe.lm.pca <- mean((df.test$siri - yhat.pca) ^ 2)

plot(df.test$siri,df.test$siri - yhat.pca)

```
Our MSPE is high and the residuals are showing a strong linear association with the response - even if we include all components as we did above.
This is definitely a problem with the code we've written.  We'll revisit this if there is time. 


### (d) Partial least squares 

```{r}
set.seed(123)
pls.mod <- plsr(siri ~., data=df.train, ncomp=5, validation ="CV") 
coefplot(pls.mod, ncomp=3, xlab="Frequency") 
plsCV <- RMSEP(pls.mod, estimate="CV") 
plot(plsCV,main="")
```

Now we find the MSPE for the test set.

```{r}

yhat.test <- predict(pls.mod,newdata = df.test, type="response")

mspe.lm.pls <- mean((df.test$siri - yhat.test) ^ 2)

```

### (e) Ridge regression 

```{r}
mean.df.full <-apply(df.train,2,mean  )
sd.df.full <-apply(df.train,2,sd)
df<-data.frame(scale(df.train, center = TRUE,scale = TRUE))
ridge.fit <- lm.ridge(siri ~., df, lambda = seq(0, .1, len=1000))
which.min(ridge.fit$GCV)
matplot(ridge.fit$lambda, coef(ridge.fit), type="l", xlab=expression(lambda) ,ylab=expression(hat(beta)),col=1)
abline(v=0.0282282282,col='red')
```

Here we predict the test set response for the ridge model.  We scaled the data before fitting the ridge model.  We display the code below for applying the scaling to the predictors, predicting the fit from the optimal model determined by cross validation, and then undoing the scaling on the predicted response.  

```{r, echo=TRUE}
df <- df.test[ , -which(names(df.test) %in% c("siri"))]
x <- as.matrix(df)


df.train.predict <- df.train[ , -which(names(df.train) %in% c("siri"))]
mean.pred.train <-apply(df.train.predict,2,mean  )
sd.pred.train <-apply(df.train.predict,2,sd)

x<- scale(x,center = mean.pred.train,scale = sd.pred.train)

yhat.ridge.scaled <- cbind(1,as.matrix(x)) %*% coef(ridge.fit)[283,]

yhat.ridge <- yhat.ridge.scaled* sd(df.train$siri) +mean(df.train$siri)

mspe.lm.ridge <- mean((df.test$siri - yhat.ridge) ^ 2)

```


### (f) Lasso regression 

```{r}
library(lars)
df <- df.train[ , -which(names(df.train) %in% c("siri"))]
x <- as.matrix(df)
lm.lasso<- lars(x,df.train$siri)
plot(lm.lasso)

cv.lm.lasso <- cv.lars(x,df.train$siri)

cv.lm.lasso$index[which.min(cv.lm.lasso$cv)]

pander(data.frame(predict(lm.lasso,s=0.8181818,type="coef",mode="fraction")$coef),caption = "Lasso Model")


df <- df.test[ , -which(names(df.test) %in% c("siri"))]
x <- as.matrix(df)
yhat.lasso <- predict(lm.lasso,x,s=0.8181818,mode="fraction")$fit

mspe.lm.lasso <- mean((df.test$siri -yhat.lasso) ^ 2)


```


### Performance Results 

```{r}
pander(data.frame(mspe.lm=mspe.lm, mspe.lm.regsubsets=mspe.lm.regsubsets,mspe.lm.pca=mspe.lm.pca,mspe.lm.pls=mspe.lm.pls,mspe.lm.ridge=mspe.lm.ridge,mspe.lm.lasso=mspe.lm.lasso),caption = "MSPE Results")

```

The PLS and ridge models performed  the best.  We are concerned about the PCA results and will debug this further. We did not get good results with the PCA using 3 components, when we allowed all components we did not see a reduction in the MSPE to the full linear model.  This is why we're concerned. Using the full set of PCA components is just a rotation of the data, so we'd expect similar MSPE results. 